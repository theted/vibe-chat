# Production Docker Compose
version: "3.8"

services:
  redis:
    image: redis:7.4-alpine
    container_name: ai-chat-redis-prod
    command: redis-server --save 60 1 --appendonly yes --loglevel warning
    volumes:
      - redis-data:/data
    restart: always
    networks:
      - ai-chat-network

  ai-chat-server:
    build:
      context: .
      dockerfile: packages/server/Dockerfile
    container_name: ai-chat-server-prod
    expose:
      - "3001"
    environment:
      - NODE_ENV=production
      - CLIENT_URL=${CLIENT_URL:-*}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GROK_API_KEY=${GROK_API_KEY}
      - GOOGLE_AI_API_KEY=${GOOGLE_AI_API_KEY}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - COHERE_API_KEY=${COHERE_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - QWEN_API_KEY=${QWEN_API_KEY}
      - KIMI_API_KEY=${KIMI_API_KEY}
      - Z_API_KEY=${Z_API_KEY}
      - REDIS_URL=redis://redis:6379
      - CHAT_ASSISTANT_SCRIPT=/app/scripts/run-mcp-chat.js
      - CHAT_ASSISTANT_AUTO_INDEX=true
      - NODE_PATH=/app/server/node_modules
      - CHAT_ASSISTANT_EMBEDDINGS_PATH=/app/.mcp-data/embeddings.json
    volumes:
      - ../scripts:/app/scripts:ro
      - ./packages/mcp-assistant:/app/mcp-assistant:ro
    restart: always
    networks:
      - ai-chat-network
    depends_on:
      - redis
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  ai-chat-client:
    build:
      context: .
      dockerfile: packages/client/Dockerfile
      args:
        - VITE_SERVER_URL=${SERVER_URL:-http://localhost:3001}
    container_name: ai-chat-client-prod
    expose:
      - "80"
    depends_on:
      - ai-chat-server
    restart: always
    networks:
      - ai-chat-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  ai-chat-network:
    driver: bridge

volumes:
  redis-data:
